{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9279d3d-fc4b-4299-95af-ef4731116120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines the\\n   proportion of variance in the dependent variable that can be explained by the independent variable.\\n     R-Squared = 1- (SSres/SStotal)\\n      where, SSres = sum of squared residual or error = sum of sq(yi-yi^)\\n             SStotal = sum of squared tatol = sum of sq(yi-mean of yi)\\n             \\n    Note: sq R can be negative\\n     '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1\n",
    "'''R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines the\n",
    "   proportion of variance in the dependent variable that can be explained by the independent variable.\n",
    "     R-Squared = 1- (SSres/SStotal)\n",
    "      where, SSres = sum of squared residual or error = sum of sq(yi-yi^)\n",
    "             SStotal = sum of squared tatol = sum of sq(yi-mean of yi)\n",
    "             \n",
    "    Note: sq R can be negative\n",
    "     '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84995365-e03f-438a-a4c0-61bd0171d4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adjusted R-squared is a corrected goodness-of-fit (model accuracy) measure for linear models. It identifies the percentage of\\n   variance in the target field that is explained by the input or inputs. R2 tends to optimistically estimate the fit of the \\n   linear regression.\\n    Adjusted R-squared = 1-(1-R^2)(1-N)/(N-P-1)\\n         where, N = number of data points\\n                P = number of independent features\\n   Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. \\n   The adjusted R-squared increases when the new term improves the model more than would be expected by chance.\\n   '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2\n",
    "'''Adjusted R-squared is a corrected goodness-of-fit (model accuracy) measure for linear models. It identifies the percentage of\n",
    "   variance in the target field that is explained by the input or inputs. R2 tends to optimistically estimate the fit of the \n",
    "   linear regression.\n",
    "    Adjusted R-squared = 1-(1-R^2)(1-N)/(N-P-1)\n",
    "         where, N = number of data points\n",
    "                P = number of independent features\n",
    "   Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. \n",
    "   The adjusted R-squared increases when the new term improves the model more than would be expected by chance.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "775f3bb0-3ad7-422d-88f1-18b7767f8e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Clearly, it is better to use Adjusted R-squared when there are multiple variables in the regression model.\\n   This would allow us to compare models with differing numbers of independent variables.\\n   Using adjusted R-squared over R-squared may be favored because of its ability to make a more accurate view of the\\n   correlation between one variable and another. Adjusted R-squared does this by taking into account how many independent\\n   variables are added to a particular model against which the stock index is measured.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3\n",
    "'''Clearly, it is better to use Adjusted R-squared when there are multiple variables in the regression model.\n",
    "   This would allow us to compare models with differing numbers of independent variables.\n",
    "   Using adjusted R-squared over R-squared may be favored because of its ability to make a more accurate view of the\n",
    "   correlation between one variable and another. Adjusted R-squared does this by taking into account how many independent\n",
    "   variables are added to a particular model against which the stock index is measured.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9094f66-a43c-4cdf-b3a9-46398537f0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' MAE evaluates the absolute distance of the observations (the entries of the dataset) to the predictions on a\\n    regression, taking the average over all observations.\\n      MAE = 1/n sum of all(i=1 to n) (yi(real) - yi(pred))    [only positive]\\n      \\n    Another way to do so is by squaring the distance, so that the results are positive. This is done by \\n    the MSE,\\n    and higher errors (or distances) weigh more in the metric than lower ones, due to the nature of the power function.\\n      MSE = 1/n sum of all(i=1 to n) (yi(real) - yi(pred))^2\\n      \\n    RMSE is used then to return the MSE error to the original unit by taking the square root of it, while maintaining\\n     the property of penalizing higher errors.\\n      RMSE = sqrt of MSE\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4\n",
    "''' MAE evaluates the absolute distance of the observations (the entries of the dataset) to the predictions on a\n",
    "    regression, taking the average over all observations.\n",
    "      MAE = 1/n sum of all(i=1 to n) (yi(real) - yi(pred))    [only positive]\n",
    "      \n",
    "    Another way to do so is by squaring the distance, so that the results are positive. This is done by \n",
    "    the MSE,\n",
    "    and higher errors (or distances) weigh more in the metric than lower ones, due to the nature of the power function.\n",
    "      MSE = 1/n sum of all(i=1 to n) (yi(real) - yi(pred))^2\n",
    "      \n",
    "    RMSE is used then to return the MSE error to the original unit by taking the square root of it, while maintaining\n",
    "     the property of penalizing higher errors.\n",
    "      RMSE = sqrt of MSE\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "647f96a9-a00f-4737-8dbe-6621877af1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Advantage of MSE:\\n    The MSE is great for ensuring that our trained model has no outlier predictions with huge errors, since the MSE puts \\n    larger weight on theses errors due to the squaring part of the function.\\n   Disadvantage of MSE:\\n     If our model makes a single very bad prediction, the squaring part of the function magnifies the error.\\n    \\n    \\n   Advantages of MAE:\\n    Our findings indicate that MAE is a more natural measure of average error, and (unlike RMSE) is unambiguous.\\n    Dimensioned evaluations and inter-comparisons of average model-performance error, therefore, should be based on MAE.\\n   Disadvantage of MAE:\\n    Although it is straightforward to interpret, MAE has a few disadvantages. For example, it doesn't tell you whether\\n    your model tends to over-estimate or under-estimate since any direction information is destroyed by taking the\\n    absolute value.\\n    \\n    \\n    Advantages of using RMSE?\\n     Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This\\n     means the RMSE is most useful when large errors are particularly undesirable. Both the MAE and RMSE can range\\n     from 0 to ∞. They are negatively-oriented scores: Lower values are better.\\n    Disadvantages of RMSE?\\n     One major drawback of RMSE is its sensitivity to outliers and the outliers have to be removed for it to function\\n     properly. RMSE increases with an increase in the size of the test sample. This is an issue when we calculate the\\n     results on different test samples. \\n     \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5\n",
    "'''Advantage of MSE:\n",
    "    The MSE is great for ensuring that our trained model has no outlier predictions with huge errors, since the MSE puts \n",
    "    larger weight on theses errors due to the squaring part of the function.\n",
    "   Disadvantage of MSE:\n",
    "     If our model makes a single very bad prediction, the squaring part of the function magnifies the error.\n",
    "    \n",
    "    \n",
    "   Advantages of MAE:\n",
    "    Our findings indicate that MAE is a more natural measure of average error, and (unlike RMSE) is unambiguous.\n",
    "    Dimensioned evaluations and inter-comparisons of average model-performance error, therefore, should be based on MAE.\n",
    "   Disadvantage of MAE:\n",
    "    Although it is straightforward to interpret, MAE has a few disadvantages. For example, it doesn't tell you whether\n",
    "    your model tends to over-estimate or under-estimate since any direction information is destroyed by taking the\n",
    "    absolute value.\n",
    "    \n",
    "    \n",
    "    Advantages of using RMSE?\n",
    "     Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This\n",
    "     means the RMSE is most useful when large errors are particularly undesirable. Both the MAE and RMSE can range\n",
    "     from 0 to ∞. They are negatively-oriented scores: Lower values are better.\n",
    "    Disadvantages of RMSE?\n",
    "     One major drawback of RMSE is its sensitivity to outliers and the outliers have to be removed for it to function\n",
    "     properly. RMSE increases with an increase in the size of the test sample. This is an issue when we calculate the\n",
    "     results on different test samples. \n",
    "     '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "973d0414-cf94-4eca-a8ab-9ce28f7a71d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. \\n   This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean.\\n\\nSimilar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty\\n  factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square.\\n  Ridge regression is also referred to as L2 Regularization.\\n  \\n  advantage of lasso regression over ridge regression, is that it produces simpler and more interpretable models that \\n  incorporate only a reduced set of the predictors.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6\n",
    "'''Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. \n",
    "   This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean.\n",
    "\n",
    "Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty\n",
    "  factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square.\n",
    "  Ridge regression is also referred to as L2 Regularization.\n",
    "  \n",
    "  advantage of lasso regression over ridge regression, is that it produces simpler and more interpretable models that \n",
    "  incorporate only a reduced set of the predictors.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fc77d3d-a1a6-4841-901a-f98eb318cee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or\\n   shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex \\n   or flexible model, avoiding the risk of Overfitting.\\n   An example is noise. Regularization is the answer to overfitting. It is a technique that improves model accuracy \\n   as well as prevents the  loss of important data due to underfitting. When a model fails to grasp an underlying data \\n   trend, it is considered to be underfitting.\\n   '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7\n",
    "'''Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or\n",
    "   shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex \n",
    "   or flexible model, avoiding the risk of Overfitting.\n",
    "   An example is noise. Regularization is the answer to overfitting. It is a technique that improves model accuracy \n",
    "   as well as prevents the  loss of important data due to underfitting. When a model fails to grasp an underlying data \n",
    "   trend, it is considered to be underfitting.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "587dc1ee-f580-44ff-ac4f-e05bbb8cbb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the limitations of linear regression model:\\n   Underfitting : A sitiuation that arises when a machine learning model fails to capture the data properly.\\n   This typically occurs when the hypothesis function cannot fit the data well.\\n\\n   Since linear regression assumes a linear relationship between the input and output varaibles, it fails to fit complex \\n   datasets properly. In most real life scenarios the relationship between the variables of the dataset isn't linear and \\n   hence a straight line doesn't fit the data properly.\\n   Its not always best choice because:\\n    Sensitive to outliers\\n    Outliers of a data set are anomalies or extreme values that deviate from the other data points of the distribution.\\n    Data outliers can damage the performance of a machine learning model drastically and can often lead to models with \\n    low accuracy.\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q8\n",
    "'''the limitations of linear regression model:\n",
    "   Underfitting : A sitiuation that arises when a machine learning model fails to capture the data properly.\n",
    "   This typically occurs when the hypothesis function cannot fit the data well.\n",
    "\n",
    "   Since linear regression assumes a linear relationship between the input and output varaibles, it fails to fit complex \n",
    "   datasets properly. In most real life scenarios the relationship between the variables of the dataset isn't linear and \n",
    "   hence a straight line doesn't fit the data properly.\n",
    "   Its not always best choice because:\n",
    "    Sensitive to outliers\n",
    "    Outliers of a data set are anomalies or extreme values that deviate from the other data points of the distribution.\n",
    "    Data outliers can damage the performance of a machine learning model drastically and can often lead to models with \n",
    "    low accuracy.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e7a39d6-97d7-4a7b-a265-9f889dc9f6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R Square is calculated by the sum of squared of prediction error divided by the total sum of the square which replaces the \\n   calculated prediction with mean. R Square value is between 0 to 1 and a bigger value indicates a better fit between \\n   prediction and actual value.\\n\\n   R Square is a good measure to determine how well the model fits the dependent variables. However, it does not take into \\n   consideration of overfitting problem. If your regression model has many independent variables, because the model is too\\n   complicated, it may fit very well to the training data but performs badly for testing data. That is why Adjusted R Square \\n   is introduced because it will penalize additional independent variables added to the model and adjust the metric to prevent\\n   overfitting issues.\\n#Example on R_Square and Adjusted R Square\\nimport statsmodels.api as sm\\nX_addC = sm.add_constant(X)\\nresult = sm.OLS(Y, X_addC).fit()\\nprint(result.rsquared, result.rsquared_adj)\\n# 0.79180307318 0.790545085707\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q9\n",
    "'''R Square is calculated by the sum of squared of prediction error divided by the total sum of the square which replaces the \n",
    "   calculated prediction with mean. R Square value is between 0 to 1 and a bigger value indicates a better fit between \n",
    "   prediction and actual value.\n",
    "\n",
    "   R Square is a good measure to determine how well the model fits the dependent variables. However, it does not take into \n",
    "   consideration of overfitting problem. If your regression model has many independent variables, because the model is too\n",
    "   complicated, it may fit very well to the training data but performs badly for testing data. That is why Adjusted R Square \n",
    "   is introduced because it will penalize additional independent variables added to the model and adjust the metric to prevent\n",
    "   overfitting issues.\n",
    "#Example on R_Square and Adjusted R Square\n",
    "import statsmodels.api as sm\n",
    "X_addC = sm.add_constant(X)\n",
    "result = sm.OLS(Y, X_addC).fit()\n",
    "print(result.rsquared, result.rsquared_adj)\n",
    "# 0.79180307318 0.790545085707\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c33c1cd3-8c47-4a02-b780-6e4448e41e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty\\nfactor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square.\\n\\nRidge regression is also referred to as L2 Regularization.\\n\\nWhen it comes to training models, there are two major problems one can encounter: overfitting and underfitting.\\n\\nOverfitting happens when the model performs well on the training set but not so well on unseen (test) data.\\nUnderfitting happens when it neither performs well on the train set nor on the test set.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q10\n",
    "'''Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a penalty\n",
    "factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the square.\n",
    "\n",
    "Ridge regression is also referred to as L2 Regularization.\n",
    "\n",
    "When it comes to training models, there are two major problems one can encounter: overfitting and underfitting.\n",
    "\n",
    "Overfitting happens when the model performs well on the training set but not so well on unseen (test) data.\n",
    "Underfitting happens when it neither performs well on the train set nor on the test set.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a012c44-fc1d-492c-8439-4eab5a9b788c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
